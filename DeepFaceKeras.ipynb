{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/15VVKl6V9F7tYkB2hcZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apyarn95/Kinship-Detection/blob/main/DeepFaceKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMk60fNL6a4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "3186e59c-6ed9-4c8e-9f9d-d6e902a03524"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! gdown https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras_vggface.vggface import VGGFace\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo\n",
            "To: /content/vgg_face_weights.h5\n",
            "580MB [00:13, 41.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZYhTmg9w6rk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "e0cb37ee-a28a-47b4-f74f-95f0fc131e56"
      },
      "source": [
        "#required libraries\n",
        "!pip install keras_vggface\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from random import choice, sample\n",
        "import cv2\n",
        "from imageio import imread\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, load_model\n",
        "from keras import regularizers\n",
        "from keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization,Dense, concatenate, Flatten, Conv1D\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from glob import glob\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D, Lambda, Reshape\n",
        "from collections import defaultdict\n",
        "from keras_vggface.utils import preprocess_input\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_vggface\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/7d/5f0319ebdc09ac1a2272364fa9583f5067b6f8aff93fbbf8835d81cbaad7/keras_vggface-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.4.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.18.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_vggface) (1.0.8)\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmCfJpzjPW7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89262528-ddc8-472d-b3df-d3e545f57208"
      },
      "source": [
        "TRAIN_BASE = '/content/drive/My Drive/train'    #folder containing training data\n",
        "families = sorted(os.listdir(TRAIN_BASE))\n",
        "print('We have {} families in the dataset'.format(len(families)))\n",
        "\n",
        "#all_images contains paths of images.\n",
        "all_images = glob(TRAIN_BASE + \"*/*/*/*.jpg\")\n",
        "\n",
        "#Splitting the data into train & validation set.\n",
        "#Validation set includes family with name F09.\n",
        "val_families = \"F09\"\n",
        "train_images = [x for x in all_images if val_families not in x]\n",
        "val_images = [x for x in all_images if val_families in x]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 470 families in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNczNJ7wqiV4"
      },
      "source": [
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAMB3td3s1Mp"
      },
      "source": [
        "#val stands for validation process\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "\n",
        "for x in train_images:\n",
        "  train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
        "\n",
        "relationships = pd.read_csv('/content/drive/My Drive/train_relationships.csv')\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
        "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_families not in x[0]]\n",
        "val = [x for x in relationships if val_families in x[0]]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vjrdIf3vJfA"
      },
      "source": [
        "# Facenet architecture will take image of size 160 x 160\n",
        "IMG_SIZE_FN = 160\n",
        "\n",
        "#Facenet architecture wil take image of size 224 x 224\n",
        "IMG_SIZE_VGG = 224\n",
        "\n",
        "input_1 = Input(shape=(IMG_SIZE_FN,IMG_SIZE_FN,3))  #facenet for image1\n",
        "input_2 = Input(shape=(IMG_SIZE_FN,IMG_SIZE_FN,3))  #facenet for image 2\n",
        "input_3 = Input(shape=(IMG_SIZE_VGG,IMG_SIZE_VGG,3))  #VGG for image 1\n",
        "input_4 = Input(shape=(IMG_SIZE_VGG,IMG_SIZE_VGG,3))  #VGG for image 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc9hKTKTwV_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b0e8962c-3815-403e-8a1e-0fe02bb4957f"
      },
      "source": [
        "#!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "from keras.models import load_model\n",
        "# load the model\n",
        "facenet_model = load_model('/content/drive/My Drive/facenet_keras.h5')\n",
        "fn_1 = facenet_model(input_1)\n",
        "fn_2 = facenet_model(input_2)\n",
        "#vgg_model = VGGFace(model='resnet50', include_top=False)\n",
        "vgg_1 = vgg_model(input_3)\n",
        "vgg_2 = vgg_model(input_4)\n",
        "    \n",
        "x1 = Reshape((1, 1 ,128))(fn_1)   #reshaping image array for global max pool layer\n",
        "x2 = Reshape((1, 1 ,128))(fn_2)\n",
        "x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "#the below 4 lamda functions will calcluate the square of each input image\n",
        "lambda_1 = Lambda(lambda tensor  : K.square(tensor))(fn_1) \n",
        "lambda_2 = Lambda(lambda tensor  : K.square(tensor))(fn_2)\n",
        "lambda_3 = Lambda(lambda tensor  : K.square(tensor))(vgg_1)\n",
        "lambda_4 = Lambda(lambda tensor  : K.square(tensor))(vgg_2)\n",
        "    \n",
        "added_facenet = Add()([x1, x2])     #this function will add two images image 1 image 2 given by facenet architecture\n",
        "added_vgg = Add()([vgg_1, vgg_2])    #this function will add two images image 3 image 4 given by VGG architecture\n",
        "subtract_fn = Subtract()([x1,x2])    #this function will subtract two images image 1 image 2 given by facenet architecture\n",
        "subtract_vgg = Subtract()([vgg_1,vgg_2])   #this function will subtract two images image 3 image 4 given by VGG architecture\n",
        "subtract_fn2 = Subtract()([x2,x1])    #this function will subtract two images image 2 image 1 given by facenet architecture\n",
        "subtract_vgg2 = Subtract()([vgg_2,vgg_1])   #this function will subtract two images image 4 image 3 given by VGG architecture\n",
        "prduct_fn1 = Multiply()([x1,x2])    #this function will multiply two images image 1 image 2 given by facenet architecture\n",
        "prduct_vgg1 = Multiply()([vgg_1,vgg_2])   #this function will multiply two images image 3 image 4 given by VGG architecture\n",
        "sqrt_fn1 = Add()([lambda_1,lambda_2])        # this function implements x1^2 + x2^2 where x1 and x2 are image by facenet\n",
        "sqrt_vgg1 = Add()([lambda_3,lambda_4])       # this function implements vgg_1^2 + vgg_2^2 where vgg_1 and vgg_2 are image by VGG\n",
        "sqrt_fn2 = Lambda(lambda tensor  : K.sign(tensor)*K.sqrt(K.abs(tensor)+1e-9))(prduct_fn1) #squre_root of sqrt_fn1\n",
        "sqrt_vgg2 = Lambda(lambda tensor  : K.sign(tensor)*K.sqrt(K.abs(tensor)+1e-9))(prduct_vgg1) #squre_root of sqrt_vgg1\n",
        "\n",
        "    \n",
        "added_vgg = Conv2D(128 , [1,1] )(added_vgg)\n",
        "subtract_vgg = Conv2D(128 , [1,1] )(subtract_vgg)\n",
        "subtract_vgg2 = Conv2D(128 , [1,1] )(subtract_vgg2)\n",
        "prduct_vgg1 = Conv2D(128 , [1,1] )(prduct_vgg1)\n",
        "sqrt_vgg1 = Conv2D(128 , [1,1] )(sqrt_vgg1)\n",
        "sqrt_vgg2 = Conv2D(128 , [1,1] )(sqrt_vgg2)\n",
        "    \n",
        "#finally concatenating all the above featues for final layer which is to be inputed to the dense layers.\n",
        "concatenated= Concatenate(axis=-1)([Flatten()(added_vgg), (added_facenet), Flatten()(subtract_vgg), (subtract_fn),\n",
        "                                   Flatten()(subtract_vgg2), (subtract_fn2), Flatten()(prduct_vgg1), (prduct_fn1), \n",
        "                                   Flatten()(sqrt_vgg1), (sqrt_fn1), Flatten()(sqrt_vgg2), (sqrt_fn2)])\n",
        "    \n",
        "concatenated= Dense(500, activation=\"relu\")(concatenated)\n",
        "concatenated= Dropout(0.1)(concatenated)\n",
        "concatenated= Dense(100, activation=\"relu\")(concatenated)\n",
        "concatenated= Dropout(0.1)(concatenated)\n",
        "concatenated= Dense(25, activation=\"relu\")(concatenated)\n",
        "concatenated= Dropout(0.1)(concatenated)\n",
        "out = Dense(1, activation=\"sigmoid\")(concatenated) #output sigmoid layer\n",
        "\n",
        "#defining the model\n",
        "model = Model([input_1, input_2, input_3, input_4], out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdtiYiWrfMhG"
      },
      "source": [
        "\n",
        "# ‘read_img_fn’ will take the path of the input image and return the same image with a \n",
        "# predefined size. Remember, we are using different input sizes of images for different\n",
        "#  base models. Like that, another function ‘read_img_vgg’ will do the same for the VGGFace \n",
        "#  model.\n",
        "\n",
        "def read_img_fn(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img,(IMG_SIZE_FN,IMG_SIZE_FN))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "#this function will read image from specified path and convert it into required size\n",
        "def read_img_vgg(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img,(IMG_SIZE_VGG,IMG_SIZE_VGG))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQY3MqS4j9ox"
      },
      "source": [
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    ppl = list(person_to_images_map.keys())\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
        "        labels = [1] * len(batch_tuples)\n",
        "        while len(batch_tuples) < batch_size:\n",
        "            p1 = choice(ppl)\n",
        "            p2 = choice(ppl)\n",
        "\n",
        "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
        "                batch_tuples.append((p1, p2))\n",
        "                labels.append(0)\n",
        "\n",
        "        for x in batch_tuples:\n",
        "            if not len(person_to_images_map[x[0]]):\n",
        "                print(x[0])\n",
        "\n",
        "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
        "        X1_FN = np.array([read_img_fn(x) for x in X1])\n",
        "        X1_VGG = np.array([read_img_vgg(x) for x in X1])\n",
        "\n",
        "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
        "        X2_FN = np.array([read_img_fn(x) for x in X2])\n",
        "        X2_VGG = np.array([read_img_vgg(x) for x in X2])\n",
        "\n",
        "        yield [X1_FN, X2_FN, X1_VGG, X2_VGG], labels  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc9R2WR_kC1I"
      },
      "source": [
        "file_path = '/content/drive/My Drive/checkpoint'\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "\n",
        "#resuming training Load checkpoint:\n",
        "import os.path\n",
        "if os.path.exists(file_path):\n",
        "    # Load model:\n",
        "    model.load_weights(file_path)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00003))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOBhCP7onyp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "576e460d-2052-4d96-fbf5-45de8d632d07"
      },
      "source": [
        "\n",
        "model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
        "                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=10, verbose=1,\n",
        "                    workers = 4,callbacks=callbacks_list, steps_per_epoch=200,validation_steps=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/200 [==============================] - 756s 4s/step - loss: 1.1224 - acc: 0.5409 - val_loss: 0.6434 - val_acc: 0.5869\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.58688, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 317s 2s/step - loss: 0.6588 - acc: 0.6269 - val_loss: 0.6198 - val_acc: 0.6338\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.58688 to 0.63375, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 228s 1s/step - loss: 0.6216 - acc: 0.6566 - val_loss: 0.6749 - val_acc: 0.6800\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.63375 to 0.68000, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 215s 1s/step - loss: 0.5940 - acc: 0.6834 - val_loss: 0.6910 - val_acc: 0.7025\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.68000 to 0.70250, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 216s 1s/step - loss: 0.5654 - acc: 0.6991 - val_loss: 0.6949 - val_acc: 0.7212\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.70250 to 0.72125, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 215s 1s/step - loss: 0.5422 - acc: 0.7191 - val_loss: 0.6333 - val_acc: 0.7412\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.72125 to 0.74125, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 215s 1s/step - loss: 0.4866 - acc: 0.7584 - val_loss: 0.6287 - val_acc: 0.7738\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.74125 to 0.77375, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 212s 1s/step - loss: 0.4787 - acc: 0.7719 - val_loss: 0.4553 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.77375 to 0.79250, saving model to /content/drive/My Drive/checkpoint\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 214s 1s/step - loss: 0.4698 - acc: 0.7725 - val_loss: 0.4058 - val_acc: 0.7531\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.79250\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 214s 1s/step - loss: 0.4474 - acc: 0.7959 - val_loss: 0.5779 - val_acc: 0.7794\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.79250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f4d3b77a4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYAdhv_6nQYv"
      },
      "source": [
        "Ksi = cv2.imread(\"/content/drive/My Drive/Ff.jpg\")\n",
        "\n",
        "Ksi_FN = cv2.resize(Ksi,(IMG_SIZE_FN,IMG_SIZE_FN))\n",
        "Ksi_FN = np.array(Ksi_FN).astype(np.float)\n",
        "Ksi_Vgg = cv2.resize(Ksi,(IMG_SIZE_VGG,IMG_SIZE_VGG))\n",
        "Ksi_Vgg = np.array(Ksi_Vgg).astype(np.float)\n",
        "X1_FN = np.array(preprocess_input(Ksi_FN,version=2))\n",
        "X1_Vgg = np.array(preprocess_input(Ksi_Vgg,version=2))\n",
        "# X1 = [x.split(\"-\")[0] for x in batch]\n",
        "# X1_FN = np.array([read_img_fn(test_path + x) for x in X1])\n",
        "# X1_VGG = np.array([read_img_vgg(test_path + x) for x in X1])\n",
        "X1_FN = np.expand_dims(X1_FN,axis=0)\n",
        "X1_Vgg = np.expand_dims(X1_Vgg,axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDPh9bPVjQPA"
      },
      "source": [
        "Fa = cv2.imread(\"/content/drive/My Drive/Logan.jpg\")\n",
        "Fa_FN = cv2.resize(Fa,(IMG_SIZE_FN,IMG_SIZE_FN))\n",
        "Fa_FN = np.array(Fa_FN).astype(np.float)\n",
        "Fa_Vgg = cv2.resize(Fa,(IMG_SIZE_VGG,IMG_SIZE_VGG))\n",
        "Fa_Vgg = np.array(Fa_Vgg).astype(np.float)\n",
        "X2_FN = np.array(preprocess_input(Fa_FN,version=2))\n",
        "X2_Vgg = np.array(preprocess_input(Fa_Vgg,version=2))\n",
        "X2_FN = np.expand_dims(X2_FN,axis=0)\n",
        "X2_Vgg = np.expand_dims(X2_Vgg,axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoASMqydKxtA"
      },
      "source": [
        "pred = model.predict([X1_FN,X2_FN,X1_Vgg,X2_Vgg])\n",
        "# model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKiF4FI-TVeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "be29f14b-89f7-460e-921a-78ee3e0918f1"
      },
      "source": [
        "Ksi.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-56becc81f121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mKsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql5AowP7TNH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0606ad9-41b3-4897-ed26-dcffd9aabeb9"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45489734]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    }
  ]
}